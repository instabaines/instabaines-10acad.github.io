
<h1>Import necessary libraries
</h1>


```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

```


```python

import pyLDAvis
import pyLDAvis.gensim  # 
import matplotlib.pyplot as plt
```


```python
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)
```


```python
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
```


```python
import random
import re
import os
import json
import time
import tweepy
import jsonpickle
import preprocessor as p
import nltk
from nltk.corpus import stopwords
import string
from textblob import TextBlob
import matplotlib.pyplot as plt
import numpy as np
#nltk.download('punkt')

from requests import get
from requests.exceptions import RequestException
from contextlib import closing
from bs4 import BeautifulSoup
import pandas as pd
import os, sys
import re
import fire
from collections import Counter
import requests
```


```python
import logging
import os
import pandas as pd
import re
import scrapy
from googlesearch import search
from scrapy.crawler import CrawlerProcess
from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor
```


```python
from textblob import TextBlob

#general text pre-processor
#!pip install nltk
import nltk
from nltk.corpus import stopwords
#nltk.download('punkt')

#tweet pre-processor 
#!pip install tweet-preprocessor
import preprocessor as p
import numpy as np

```


```python
import re
import numpy as np
import pandas as pd
from pprint import pprint

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# spacy for lemmatization
import spacy
```

    C:\Users\Hp\Anaconda3\lib\site-packages\gensim\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial
      warnings.warn("detected Windows; aliasing chunkize to chunkize_serial")
    

<h1>Define a webcrawler</h1>


```python
#%%writefile ../pyscrap_url.py

def simple_get(url):
    """
    Attempts to get the content at `url` by making an HTTP GET request.
    If the content-type of response is some kind of HTML/XML, return the
    text content, otherwise return None.
    """
    try:
        with closing(get(url, stream=True)) as resp:
            if is_good_response(resp):
                return resp.content  #.encode(BeautifulSoup.original_encoding)
            else:
                return None

    except RequestException as e:
        log_error('Error during requests to {0} : {1}'.format(url, str(e)))
        return None


def is_good_response(resp):
    """
    Returns True if the response seems to be HTML, False otherwise.
    """
    content_type = resp.headers['Content-Type'].lower()
    return (resp.status_code == 200 
            and content_type is not None 
            and content_type.find('html') > -1)


def log_error(e):
    """
    It is always a good idea to log errors. 
    This function just prints them, but you can
    make it do anything.
    """
    print(e)
    
def get_elements(url, tag='h2',search={}, fname=None):
    """
    Downloads a page specified by the url parameter
    and returns a list of strings, one per tag element
    """
    
    if isinstance(url,str):
        response = simple_get(url)
    else:
        #if already it is a loaded html page
        response = url

    if response is not None:
        html = BeautifulSoup(response, 'html.parser')
        
        res = []
        if tag:    
            for li in html.select(tag):
                for name in li.text.split('\n'):
                    if len(name) > 0:
                        res.append(name.strip())
                       
                
        if search:
            soup = html            
            
            
            r = ''
            if 'find' in search.keys():
                print('findaing',search['find'])
                soup = soup.find(**search['find'])
                r = soup

                
            if 'find_all' in search.keys():
                print('findaing all of',search['find_all'])
                r = soup.find_all(**search['find_all'])
   
            if r:
                for x in list(r):
                    if len(x) > 0:
                        res.extend(x)
            
        return res

    # Raise an exception if we failed to get any data from the url
    raise Exception('Error retrieving contents at {}'.format(url))    
    
    
if get_ipython().__class__.__name__ == '__main__':
    fire(get_elements)
```


```python
def get_urls(tag,n,language):
    urls=[url for url in search(tag,stop=n,lang=language,pause=2.0)][:n]
    return urls
          
```


```python
url_journalist=get_urls('Twitter handles of top journalist in Nigeria',100,'en')
```


```python
url_government=get_urls('Twitter handles of government officials in Nigeria',100,'en')

```


```python
url_celebrities=get_urls('Twitter handles of celebrities in Nigeria',100,'en')
```


```python
url_celebrities
```


```python
for url in url_celebrities:
    if 'twitter.com' in url:
        print (url)
```


```python
##(@\w+)|
a=re.search('(?:https?:\/\/)?(?:www\.)?twitter\.com\/(?:#!\/)?@?([^\/\?\s]*)',str(url_government[1]))
print (a[1])
```


```python
journalist=[]
for i in url_journalist:
    search_=re.search('(?:https?:\/\/)?(?:www\.)?twitter\.com\/(?:#!\/)?@?([^\/\?\s]*)',str(i))
    if search_ !=None:
        journalist.append(search_[1])
```


```python
govt_official=[]
for i in url_government:
    search1=re.search('(?:https?:\/\/)?(?:www\.)?twitter\.com\/(?:#!\/)?@?([^\/\?\s]*)',str(i))
    if search1 !=None:
        govt_official.append(search1[1])
```


```python
govt_official
```


```python
url= 'https://www.nairaland.com/1166340/naija-celebrities-twitter-handles'
response = simple_get(url)

res = get_elements(response, search={'find_all':{'class_':'narrow'}})
res
```

    findaing all of {'class_': 'narrow'}
    




    ['Naija Celebrities Twitter Handles - Entertainment - Nairaland',
     "Here is a list of some of Nigeria's celebrities twitter handle",
     <br/>,
     <br/>,
     'Wizkid = @Wizkidayo',
     <br/>,
     'Ice Prince = @Iceprincezamani',
     <br/>,
     'Banky W = @BankyW',
     <br/>,
     'Cossy = @Cossydiva',
     <br/>,
     'Tonto Dikeh = @TONTOLET',
     <br/>,
     'Olamide = @olamide_YBNL',
     <br/>,
     'Don Jazzy = @DONJAZZY',
     <br/>,
     "D'banj = @iamdbanj",
     <br/>,
     'Tiwa Savage = @TiwaSavage',
     <br/>,
     'Omotola = @Realomosexy',
     <br/>,
     'Rita Dominic = @ritaUdominic',
     <br/>,
     'Uche Jombo = @uchejombo',
     <br/>,
     'Davido = @iam_Davido',
     <br/>,
     'Phyno = @phynofino',
     <br/>,
     'Wande Coal = @wandecoal',
     <br/>,
     <br/>,
     <br/>,
     'And finally my own @RealStaceyBanks or ',
     <a href="http://www.twitter.com/realstaceybanks">www.twitter.com/realstaceybanks</a>,
     <br/>,
     <br/>,
     <br/>,
     'Thank You',
     ' ',
     <iframe allowfullscreen="" class="youtube" frameborder="0" src="https://www.youtube.com/embed/gEAfdlpKVQw"></iframe>,
     <br/>,
     <a href="https://www.youtube.com/watch?v=gEAfdlpKVQw">https://www.youtube.com/watch?v=gEAfdlpKVQw</a>,
     ' ',
     <iframe allowfullscreen="" class="youtube" frameborder="0" src="https://www.youtube.com/embed/avmMplwo688"></iframe>,
     <br/>,
     <a href="https://www.youtube.com/watch?v=avmMplwo688">https://www.youtube.com/watch?v=avmMplwo688</a>,
     ' ',
     <iframe allowfullscreen="" class="youtube" frameborder="0" src="https://www.youtube.com/embed/Erx2fqnZ7xI"></iframe>,
     <br/>,
     <a href="https://www.youtube.com/watch?v=Erx2fqnZ7xI">https://www.youtube.com/watch?v=Erx2fqnZ7xI</a>,
     ' ',
     <iframe allowfullscreen="" class="youtube" frameborder="0" src="https://www.youtube.com/embed/l2kZIkSK5Cw"></iframe>,
     <br/>,
     <a href="https://www.youtube.com/watch?v=l2kZIkSK5Cw">https://www.youtube.com/watch?v=l2kZIkSK5Cw</a>,
     'BECOME TECTONO LADY OF THE WEEK',
     <br/>,
     'Tectono Business Review is the Nigerian version of Harvard Business Review, Oxford Business Group, Financial Times, The Economist, EuroMoney etc. It publishes business news, business articles and adverts. The main people that read our publications several times in a day are the elites such as business tycoons, top politicians, captains of industries, heads of federal parastatals, CEOs and top managers of multinational companies. We publish via our website: ',
     <a href="http://www.tectono-business.com/">www.tectono-business.com/</a>,
     ' We would like you to be the face of our publishing firm for the week? It is a weekly contest.',
     <br/>,
     <br/>,
     'So, do you consider yourself beautiful, educated, intelligent and worthy of being published in the most sought after business consultancy, research and publishing website worldwide? Do you want your profession, skills and all your good qualities to be showcased all over the world? Then, apply to become Tectono Lady of the Week.',
     <br/>,
     <br/>,
     'Yes, anyone who wins Tectono Lady of the Week will have her picture, phone number (optional), profession and a full description of herself published in Tectono Business Review and shared on all social media platforms. Do you know what it means for millions of people to know you, what you do and what you intend to do in the future and also contact you? It is the easiest avenue of becoming a superstar in your own field of endeavor and also prosperous. To apply, click the link below and follow the instruction.',
     <br/>,
     <a href="http://www.tectono-business.com/2016/01/become-tectono-lady-of-week.html">http://www.tectono-business.com/2016/01/become-tectono-lady-of-week.html</a>]




```python
re.search('@.*',res[6])[0]
```




    '@Iceprincezamani'




```python
celeb=[]
for i in res:
    search1=re.search('@.*',str(i))
    if search1 !=None:
        celeb.append(search1[0])
```


```python
del celeb[-1]
celeb.append('@RealStaceyBanks')
celeb
```




    ['@Wizkidayo',
     '@Iceprincezamani',
     '@BankyW',
     '@Cossydiva',
     '@TONTOLET',
     '@olamide_YBNL',
     '@DONJAZZY',
     '@iamdbanj',
     '@TiwaSavage',
     '@Realomosexy',
     '@ritaUdominic',
     '@uchejombo',
     '@iam_Davido',
     '@phynofino',
     '@wandecoal',
     '@RealStaceyBanks']




```python
consumer_key = '##############'
consumer_secret = '##############'
access_token = '##############'
access_token_secret = '##############'
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)

```


```python

```


```python
new_user=[]
for i in celeb:
    try:
        for user in tweepy.Cursor(api.friends, screen_name=i,wait_on_rate_limit=True).items():
                if 'Nigeria' in user.location:
                    new_user.append(user.screen_name)
        #time.sleep(60)
    except tweepy.TweepError as ex:
        if ex.reason == "Not authorized" :
            continue
```


```python
a_user=[]
for i in journalist:
    try:
        for user in tweepy.Cursor(api.friends, screen_name=i,wait_on_rate_limit=True).items():
                #print (user)
                if (user.followers_count)>200:
                    a_user.append(user.screen_name)
                #time.sleep(60)
    except tweepy.TweepError as ex:
        if ex.reason == "Not authorized" :
            continue
```


```python
b_user=[]
for i in govt_official:
    #try:
        for user in tweepy.Cursor(api.friends, screen_name=i,wait_on_rate_limit=True).items():
                print (user)
                if (user.followers_count)>200:
                    b_user.append(user.screen_name)
        #time.sleep(60)
    #except tweepy.TweepError as ex:
     #   if ex.reason == "Not authorized" :
      #      continue
```


```python
final_list=a_user+b_user+celeb+govt_official+journalist

```

<h1>Twitter crawler</h1>
<p>This crawler use Twitter API to fecth data from Twitter</p>


```python
class tweetsearch():
    '''
    This is a basic class to search and download twitter data.
    You can build up on it to extend the functionalities for more 
    sophisticated analysis
    '''
    def __init__(self, cols=None,auth=None):
        #
        if not cols is None:
            self.cols = cols
        else:
            self.cols = ['id', 'created_at', 'source', 'original_text','clean_text', 
                    'sentiment','polarity','subjectivity', 'lang',
                    'favorite_count', 'retweet_count', 'original_author',   
                    'possibly_sensitive', 'hashtags',
                    'user_mentions', 'place', 'place_coord_boundaries']
            
        if auth is None:
            consumer_key = 'haAX9mGU6dfO5l9WXhilBNT1z'
            consumer_secret = 'tBCRdcAcjAnyZizPRqAlb2R38bDOEia3W4sOsMxrty9FRJnF8j'
            access_token = '129153692-WlHYMMwfRCsLjrshrLjcZPE0scqDtlycks9o6whO'
            access_token_secret = 'omoGw2DW7yItrnUrVT4pKfnK6vddSBuJzkYo8qHHmy5LL'
            auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
            auth.set_access_token(access_token, access_token_secret)
            
        


            #This handles Twitter authetification and the connection to Twitter Streaming API
            
            

        #            
        self.auth = auth
        self.api = tweepy.API(auth) 
        self.filtered_tweet = ''
            

    def clean_tweets(self, twitter_text):

        #use pre processor
        tweet = p.clean(twitter_text)

         #HappyEmoticons
        emoticons_happy = set([
            ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',
            ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',
            '=-3', '=3', ':-))', ":'-)", ":')", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',
            'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',
            '<3'
            ])

        # Sad Emoticons
        emoticons_sad = set([
            ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',
            ':-[', ':-<', '=\\', '=/', '>:(', ':(', '>.<', ":'-(", ":'(", ':\\', ':-c',
            ':c', ':{', '>:\\', ';('
            ])

        #Emoji patterns
        emoji_pattern = re.compile("["
                 u"\U0001F600-\U0001F64F"  # emoticons
                 u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                 u"\U0001F680-\U0001F6FF"  # transport & map symbols
                 u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                 u"\U00002702-\U000027B0"
                 u"\U000024C2-\U0001F251"
                 "]+", flags=re.UNICODE)

        #combine sad and happy emoticons
        emoticons = emoticons_happy.union(emoticons_sad)

        stop_words = set(stopwords.words('english'))
        word_tokens = nltk.word_tokenize(tweet)
        #after tweepy preprocessing the colon symbol left remain after      
        #removing mentions
        tweet = re.sub(r':', '', tweet)
        tweet = re.sub(r'‚Ä¶', '', tweet)

        #replace consecutive non-ASCII characters with a space
        tweet = re.sub(r'[^\x00-\x7F]+',' ', tweet)

        #remove emojis from tweet
        tweet = emoji_pattern.sub(r'', tweet)

        #filter using NLTK library append it to a string
        filtered_tweet = [w for w in word_tokens if not w in stop_words]

        #looping through conditions
        filtered_tweet = []    
        for w in word_tokens:
        #check tokens against stop words , emoticons and punctuations
            if w not in stop_words and w not in emoticons and w not in string.punctuation:
                filtered_tweet.append(w)

        return ' '.join(filtered_tweet)            
    def find_friends(self,i):
        test=set()
        try:
            for user in tweepy.Cursor(api.friends, screen_name=i,wait_on_rate_limit=True,geocode='6.465422,3.406448,1km').items():
                #print (user)
                if (user.statuses_count)>500:
                    test.add(user.screen_name)
        except tweepy.TweepError as e:
            pass
            print (e)
        return test
                    
    def get_tweets(self, keyword, csvfile=None):
        
        
        df = pd.DataFrame(columns=self.cols)
        
        if not csvfile is None:
            #If the file exists, then read the existing data from the CSV file.
            if os.path.exists(csvfile):
                df = pd.read_csv(csvfile, header=0)
            

        #page attribute in tweepy.cursor and iteration
        for status in tweepy.Cursor(self.api.user_timeline, screen_name=keyword, include_rts=False,wait_on_rate_limit=True,tweet_mode='extended').items(505):

            # the you receive from the Twitter API is in a JSON format and has quite an amount of information attached
            #for status in page:
                
                new_entry = []
                status = status._json
                
                #filter by language
                if status['lang'] != 'en':
                    continue

                
                #if this tweet is a retweet update retweet count
                if status['created_at'] in df['created_at'].values:
                    i = df.loc[df['created_at'] == status['created_at']].index[0]
                    #
                    cond1 = status['favorite_count'] != df.at[i, 'favorite_count']
                    cond2 = status['retweet_count'] != df.at[i, 'retweet_count']
                    if cond1 or cond2:
                        df.at[i, 'favorite_count'] = status['favorite_count']
                        df.at[i, 'retweet_count'] = status['retweet_count']
                    continue

                #calculate sentiment
                filtered_tweet = self.clean_tweets(status['full_text'])
                blob = TextBlob(filtered_tweet)
                Sentiment = blob.sentiment     
                polarity = Sentiment.polarity
                subjectivity = Sentiment.subjectivity

                new_entry += [status['id'], status['created_at'],
                              status['source'], status['full_text'], filtered_tweet, 
                              Sentiment,polarity,subjectivity, status['lang'],
                              status['favorite_count'], status['retweet_count']]

                new_entry.append(status['user']['screen_name'])

                try:
                    is_sensitive = status['possibly_sensitive']
                except KeyError:
                    is_sensitive = None

                new_entry.append(is_sensitive)

                hashtags = ", ".join([hashtag_item['text'] for hashtag_item in status['entities']['hashtags']])
                new_entry.append(hashtags) #append the hashtags

                #
                mentions = ", ".join([mention['screen_name'] for mention in status['entities']['user_mentions']])
                new_entry.append(mentions) #append the user mentions

                try:
                    xyz = status['place']['bounding_box']['coordinates']
                    coordinates = [coord for loc in xyz for coord in loc]
                except TypeError:
                    coordinates = None
                #
                new_entry.append(coordinates)

                try:
                    location = status['user']['location']
                except TypeError:
                    location = ''
                #
                new_entry.append(location)

                #now append a row to the dataframe
                single_tweet_df = pd.DataFrame([new_entry], columns=self.cols)
                df = df.append(single_tweet_df, ignore_index=True)

        if not csvfile is None:
            #save it to file
            df.to_csv(csvfile, columns=self.cols, index=False, encoding="utf-8")
            
        return df
```


```python
#final_list=set(list(test)+final_list)

```


```python
ts=tweetsearch()
```


```python
test=[]
for i in final_list:
    if (len(final_list)+len(final_list))<1000:
        test.extend(ts.find_friends(i))
final_list.extend(test)
```


```python
for i in final_list:
    try:
        a=ts.get_tweets(i)
        a['user']=i
        I_A=pd.concat([a,I_A])
    except NameError:
        I_A=a
    except tweepy.TweepError as e:
        print (e)
        pass
```


```python
#final_list=pd.DataFrame({'final_list':final_list})
#final_list.to_csv('final.csv')
#final_list=pd.read_csv('./final.csv')
#final_list=list(final_list.final_list)

```


```python
df=I_A
len(df.user.unique())
```

    C:\Users\Hp\Anaconda3\lib\site-packages\IPython\core\interactiveshell.py:3057: DtypeWarning: Columns (1,8,10,11,19) have mixed types. Specify dtype option on import or set low_memory=False.
      interactivity=interactivity, compiler=compiler, result=result)
    




    959



<p>Cleaning the original text and preparing for LDA</p>


```python
data = df.original_text.values.tolist()
data = [re.sub('\S*@\S*\s?', '', str(sent)) for sent in data]
data = [re.sub("\'", "", str(sent)) for sent in data]
data

```


```python
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

data_words = list(sent_to_words(data))

print(data_words[:1])
```

    [['truly', 'is']]
    


```python
bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[data_words], threshold=100) 
```

    C:\Users\Hp\Anaconda3\lib\site-packages\gensim\models\phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class
      warnings.warn("For a faster implementation, use the gensim.models.phrases.Phraser class")
    


```python
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)

# See trigram example
print(trigram_mod[bigram_mod[data_words[0]]])
```

    ['truly', 'is']
    

<b>Necessary fuctions for preparing the necessary data for LDA modelling are provided here</b>


```python
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out
```


```python
# Remove Stop Words
data_words_nostops = remove_stopwords(data_words)

# Form Bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# Initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en', disable=['parser', 'ner'])

# Do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

print(data_lemmatized[:1])
```

    [['truly']]
    


```python
# Create Dictionary
id2word = corpora.Dictionary(data_lemmatized)

# Create Corpus
texts = data_lemmatized

# Term Document Frequency
corpus = [id2word.doc2bow(text) for text in texts]

# View
print(corpus[:1])
```

    [[(0, 1)]]
    


```python
id2word[0]
```




    'truly'



<big>The LDA model is define below</big>


```python
lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                           id2word=id2word,
                                           num_topics=4, 
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=True)
```


```python
#Print the Keyword in the 4 topics
print(lda_model.print_topics())
doc_lda = lda_model[corpus]
```

    [(0, '0.025*"thank" + 0.021*"today" + 0.017*"good" + 0.017*"year" + 0.014*"see" + 0.014*"great" + 0.013*"day" + 0.013*"take" + 0.011*"http" + 0.010*"read"'), (1, '0.086*"co" + 0.054*"https" + 0.023*"covid" + 0.017*"support" + 0.014*"say" + 0.013*"go" + 0.012*"need" + 0.012*"get" + 0.011*"country" + 0.011*"woman"'), (2, '0.021*"people" + 0.019*"work" + 0.019*"new" + 0.018*"make" + 0.011*"well" + 0.010*"global" + 0.010*"must" + 0.009*"community" + 0.009*"research" + 0.008*"first"'), (3, '0.032*"amp" + 0.020*"time" + 0.014*"learn" + 0.014*"call" + 0.012*"want" + 0.011*"report" + 0.010*"life" + 0.009*"share" + 0.008*"education" + 0.008*"case"')]
    


```python
social=["thank","today","good","year","see","great","day","take","read"]
economy=["people","work","new","make","well","global","must","community","research","first"]
health=["covid","support",'say',"go","need","get","country","woman"]
cultural=["time","learn","call","want","report","life","share","education","case"]
```


```python

```


```python
fg=['i','88']
yu='i am a dog'
matchers = ['abc','def']
matching = [s for s in fg if(s in yu)]
if matching:
    print (len(matching))
```

    1
    


```python
social=["thank","today","good","year","see","great","day","take","read"] #0
economy=["people","work","new","make","well","global","must","community","research","first"] #1
health=["covid","support",'say',"go","need","get","country","woman"] #2
cultural=["time","learn","call","want","report","life","share","education","case"] #3 or education
df.columns
```




    Index(['Unnamed: 0', 'id', 'created_at', 'source', 'original_text',
           'clean_text', 'sentiment', 'polarity', 'subjectivity', 'lang',
           'favorite_count', 'retweet_count', 'original_author',
           'possibly_sensitive', 'hashtags', 'user_mentions', 'place',
           'place_coord_boundaries', 'user', 'Unnamed: 0.1'],
          dtype='object')



The following cells registers vote according to the defined keywords of the topics as stated above


```python
vote=[]
for i in df.index:
    x=str(df.at[i,'original_text'])
    #print (x)
    matching=[s for s in x if(s in social)]
    if matching:
        vote.append(0)
    else:
        matching=[s for s in x if(s in economy)]
        if matching:
            vote.append(1)
        else:
            matching=[s for s in x if(s in health)]
            if matching:
                vote.append(2)
            else:
                matching=[s for s in x if(s in cultural)]
                if matching:
                    vote.append(3)
                else:
                    vote.append(random.randint(0, 3))
```


```python
len(vote)
```




    377694




```python
df['topic']=vote

```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>id</th>
      <th>created_at</th>
      <th>source</th>
      <th>original_text</th>
      <th>clean_text</th>
      <th>sentiment</th>
      <th>polarity</th>
      <th>subjectivity</th>
      <th>lang</th>
      <th>...</th>
      <th>retweet_count</th>
      <th>original_author</th>
      <th>possibly_sensitive</th>
      <th>hashtags</th>
      <th>user_mentions</th>
      <th>place</th>
      <th>place_coord_boundaries</th>
      <th>user</th>
      <th>Unnamed: 0.1</th>
      <th>topic</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>1.2869e+18</td>
      <td>Sat Jul 25 05:24:59 +0000 2020</td>
      <td>&lt;a href="http://twitter.com/download/android" ...</td>
      <td>@DewloUKnow @Amy_Siskind Truly is</td>
      <td>Truly</td>
      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>
      <td>0.0000</td>
      <td>0</td>
      <td>en</td>
      <td>...</td>
      <td>0</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>DewloUKnow, Amy_Siskind</td>
      <td>NaN</td>
      <td>Whittier</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1.28689e+18</td>
      <td>Sat Jul 25 05:12:32 +0000 2020</td>
      <td>&lt;a href="http://twitter.com/download/android" ...</td>
      <td>@alternabirth @chrissyteigen My information co...</td>
      <td>My information comes helping physicians fill d...</td>
      <td>Sentiment(polarity=0.0, subjectivity=0.0666666...</td>
      <td>0.0000</td>
      <td>0.0666667</td>
      <td>en</td>
      <td>...</td>
      <td>0</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>alternabirth, chrissyteigen</td>
      <td>NaN</td>
      <td>Whittier</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>1.28686e+18</td>
      <td>Sat Jul 25 03:03:17 +0000 2020</td>
      <td>&lt;a href="http://twitter.com/download/android" ...</td>
      <td>@VioletRiotGames Yes my Dad taught me</td>
      <td>Yes Dad taught</td>
      <td>Sentiment(polarity=0.0, subjectivity=0.0)</td>
      <td>0.0000</td>
      <td>0</td>
      <td>en</td>
      <td>...</td>
      <td>0</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>VioletRiotGames</td>
      <td>NaN</td>
      <td>Whittier</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>1.2868e+18</td>
      <td>Fri Jul 24 22:51:53 +0000 2020</td>
      <td>&lt;a href="http://twitter.com/download/android" ...</td>
      <td>@odranwaldo I believe my brain is loud enough ...</td>
      <td>I believe brain loud enough already thanks</td>
      <td>Sentiment(polarity=0.10000000000000002, subjec...</td>
      <td>0.1000</td>
      <td>0.5</td>
      <td>en</td>
      <td>...</td>
      <td>0</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>odranwaldo</td>
      <td>NaN</td>
      <td>Whittier</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>1.2868e+18</td>
      <td>Fri Jul 24 22:50:36 +0000 2020</td>
      <td>&lt;a href="http://twitter.com/download/android" ...</td>
      <td>@DrPnygard @MollyJongFast @SmileItsNathan Yes ...</td>
      <td>Yes precisely I zoomed original</td>
      <td>Sentiment(polarity=0.3875, subjectivity=0.775)</td>
      <td>0.3875</td>
      <td>0.775</td>
      <td>en</td>
      <td>...</td>
      <td>0</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>DrPnygard, MollyJongFast, SmileItsNathan</td>
      <td>NaN</td>
      <td>Whittier</td>
      <td>smora75</td>
      <td>NaN</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 21 columns</p>
</div>




```python
user=df.user.unique()
topic0=[]
topic1=[]
topic2=[]
topic3=[]
for i in user:
    topic0=len(df.loc[(df['user']==i) &  (df['topic']==0)][['user','topic']])
    topic1=len(df.loc[(df['user']==i) &  (df['topic']==1)][['user','topic']])
    topic2=len(df.loc[(df['user']==i) &  (df['topic']==2)][['user','topic']])
    topic3=len(df.loc[(df['user']==i) &  (df['topic']==3)][['user','topic']])
    dict1={'user':i,'topic0':topic0,'topic1':topic1,'topic2':topic2,'topic3':topic3}
    df1=pd.DataFrame(dict1, index=[0])
    try:
        A=pd.concat([df1,A])
    except NameError:
        A=df1
```


```python
#A.to_csv('df_topic.csv')
```

<big>Custering of data using kMeans Algorithm to reveal the relationship among communities</big>


```python
x=A[['topic0','topic1','topic2','topic3']]
scaler=StandardScaler().fit(x)
newx=scaler.transform(x)
kmax = 10
sil=[]

# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2
for k in range(2, kmax+1):
    kmeans = KMeans(n_clusters = k).fit(newx)
    labels = kmeans.labels_
    sil.append(silhouette_score(newx, labels, metric = 'euclidean'))

rangex=range(2,kmax+1)
fig,ax=plt.subplots()
ax.plot(rangex,sil)
ax.set_xlabel('No of Cluster', size=10)
ax.set_ylabel('Silhouettescore', size=10)
fig.savefig('sil plot.jpg')
```


![png](Twittercommunities_files/Twittercommunities_64_0.png)



```python
kmeans = KMeans(n_clusters = 2).fit(x)
```


```python

```


```python

fig, ax = plt.subplots(1, 1, figsize=(10, 5))
# using two cluster centers:
kmeans = KMeans(n_clusters=2)
kmeans.fit(x)
ax.scatter(A['topic0'], A['topic2'], c=kmeans.labels_, marker='o')
ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],marker='^',c='r',s=[120,150])
ax.set_xlabel('Social', size=10)
ax.set_ylabel('Economy',size=10)
fig.savefig('cluster.jpg')
```


![png](Twittercommunities_files/Twittercommunities_67_0.png)



```python
predict=kmeans.predict(x)
A['cluster']=predict
#A.to_csv('A.to_csv')
```


```python
del A['Unnamed: 0']
del df['Unnamed: 0']
```


```python
newdf=pd.merge(A,df,on='user')
```


```python
len(A.loc[A['cluster']==0])/len(A)
```




    0.8185610010427529




```python
len(A.loc[A['cluster']==1])/len(A)
```




    0.18143899895724713




```python
A1=A.loc[A['cluster']==1]
A0=A.loc[A['cluster']==0]
```


```python
list(df.hashtags.unique())

```


```python
#df.loc[df['hashtags']==re'(?:https?:\/\/)?(?:www\.)COVID']
100*(len(df[df['hashtags'].str.contains('(Vaccines|StayHome|stayhome|Covid|LetStaySafe|COVID|WearAMask)', regex=True,na=False)][['user','hashtags']])/len(df))
```


```python
df_covid=df[df['hashtags'].str.contains('(Vaccines|StayHome|stayhome|Covid|LetStaySafe|COVID|WearAMask)', regex=True,na=False)][['user','hashtags']]

```


```python
len(df_covid['user']==A0[['user']])/len(df)
```


```python
100*(len(df[df['hashtags'].str.contains('(Nigeria|naija|9ja)', 
                                    regex=True,na=False)][['user','hashtags']])/len(df))
```
